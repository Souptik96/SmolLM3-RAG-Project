{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:57:14.732544Z","iopub.execute_input":"2025-07-23T07:57:14.732795Z","iopub.status.idle":"2025-07-23T07:57:15.023996Z","shell.execute_reply.started":"2025-07-23T07:57:14.732775Z","shell.execute_reply":"2025-07-23T07:57:15.023434Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Workflow:\nevery time I stop the runtime:\n\n1) I install the dependencies,\n\n2) restart the session\n\n3) start from model_loader.py up till # Now run the app !python app.py","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y numpy torch torchao torchvision torchaudio transformers sentence-transformers peft accelerate > /dev/null 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:54:17.868780Z","iopub.execute_input":"2025-07-23T07:54:17.868981Z","iopub.status.idle":"2025-07-23T07:54:41.603305Z","shell.execute_reply.started":"2025-07-23T07:54:17.868952Z","shell.execute_reply":"2025-07-23T07:54:41.602475Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install numpy==1.24.4 > /dev/null 2>&1\n!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu121 > /dev/null 2>&1\n!pip install --upgrade transformers > /dev/null 2>&1\n!pip install bitsandbytes==0.44.1 > /dev/null 2>&1\n!pip install sentence-transformers==2.7.0 > /dev/null 2>&1\n!pip install accelerate==0.33.0 > /dev/null 2>&1\n!pip install peft==0.15.2 diskcache faiss-cpu -U ddgs > /dev/null 2>&1\n!pip install rank_bm25 > /dev/null 2>&1\n!pip install nltk > /dev/null 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:54:41.607134Z","iopub.execute_input":"2025-07-23T07:54:41.607347Z","iopub.status.idle":"2025-07-23T07:56:55.449777Z","shell.execute_reply.started":"2025-07-23T07:54:41.607326Z","shell.execute_reply":"2025-07-23T07:56:55.448988Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nos._exit(00)  # Restart the kernel","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-23T07:57:08.637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pkg_resources\n\n# Re-defining the list since the code environment was reset\npackages_to_check = [\n    \"numpy\",\n    \"torch\",\n    \"torchvision\",\n    \"torchaudio\",\n    \"transformers\",\n    \"sentence-transformers\",\n    \"bitsandbytes\",\n    \"accelerate\",\n    \"peft\",\n    \"diskcache\",\n    \"faiss-cpu\",\n    \"ddgs\",\n    \"rank_bm25\"\n]\n\ninstalled_versions = {\n    pkg: pkg_resources.get_distribution(pkg).version for pkg in packages_to_check if pkg_resources.working_set.by_key.get(pkg)\n}\n\ninstalled_versions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile model_loader.py\n\nimport torch\nimport os\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login, snapshot_download\nfrom sentence_transformers import SentenceTransformer\n\n\ndef load_model_and_tokenizer(model_name=\"soupstick/smollm3-qlora-ft\"):\n    \"\"\"Load model with robust tokenizer handling\"\"\"\n    \n    hf_token = os.environ.get(\"HF_TOKEN\", \"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\")\n    login(token=hf_token)\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    # Clear tokenizer cache\n    os.system(\"rm -rf ~/.cache/huggingface/tokenizers\")\n    \n    # Download fresh model files\n    model_path = snapshot_download(\n        repo_id=model_name,\n        token=hf_token,\n        ignore_patterns=[\"*.bin\", \"*.safetensors\"]  # Skip weights, we'll load separately\n    )\n    \n    # Load tokenizer first with error handling\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_path,\n            token=hf_token,\n            trust_remote_code=True\n        )\n    except Exception as e:\n        print(f\"Tokenizer loading failed: {e}\")\n        # Fallback to GPT2 tokenizer as temporary measure\n        from transformers import GPT2Tokenizer\n        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n    # Model config\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16\n    )\n    \n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",\n        quantization_config=bnb_config,\n        torch_dtype=torch.float16,\n        token=hf_token,\n        trust_remote_code=True\n    )\n    \n    # Resize token embeddings if using fallback tokenizer\n    if tokenizer.vocab_size != model.config.vocab_size:\n        model.resize_token_embeddings(len(tokenizer))\n    \n    # Optimization\n    torch.backends.cuda.enable_flash_sdp(True)\n    torch.set_float32_matmul_precision(\"high\")\n    if device == \"cuda\":\n        model = torch.compile(model)\n    \n    return model, tokenizer, device\n\ndef load_embedder():\n    return SentenceTransformer(\"all-MiniLM-L6-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:57:22.308890Z","iopub.execute_input":"2025-07-23T07:57:22.309299Z","iopub.status.idle":"2025-07-23T07:57:22.315360Z","shell.execute_reply.started":"2025-07-23T07:57:22.309279Z","shell.execute_reply":"2025-07-23T07:57:22.314578Z"}},"outputs":[{"name":"stdout","text":"Writing model_loader.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile search_engine.py\n\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict, Optional\nfrom ddgs import DDGS\nimport hashlib\nimport pickle\nfrom diskcache import Cache\nfrom rank_bm25 import BM25Okapi\nfrom sentence_transformers import SentenceTransformer\nfrom vector_db import VectorDB\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\n\nclass HybridSearchEngine:\n    def __init__(self, embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n        self.embedder = SentenceTransformer(embedding_model_name)\n        self.vector_db = VectorDB(self.embedder)\n        self.bm25 = None\n        self.documents = []\n        self.raw_results = []\n        self.cache = Cache('/kaggle/working/search_cache')\n        \n    def _hash_query(self, query: str) -> str:\n        return hashlib.sha256(pickle.dumps(query)).hexdigest()\n    \n    def _tokenize(self, text: str) -> List[str]:\n        return word_tokenize(text.lower())\n    \n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    def web_search(self, query: str, max_results: int = 5, cutoff_days: int = 30) -> List[Dict]:\n        cache_key = self._hash_query(query)\n        if cache_key in self.cache:\n            return self.cache[cache_key]\n            \n        cutoff_date = datetime.now().date() - timedelta(days=cutoff_days)\n        \n        try:\n            with DDGS() as ddg:\n                results = ddg.text(query, max_results=max_results)\n                filtered = []\n                \n                for r in results:\n                    result_date = None\n                    if \"date\" in r and r[\"date\"]:\n                        try:\n                            result_date = datetime.strptime(r[\"date\"], \"%Y-%m-%d\").date()\n                        except ValueError:\n                            pass\n                    \n                    if not result_date or result_date >= cutoff_date:\n                        filtered.append({\n                            \"title\": r.get(\"title\", \"\"),\n                            \"url\": r.get(\"href\", \"\"), \n                            \"snippet\": r.get(\"body\", \"\"),\n                            \"date\": result_date\n                        })\n                \n                self.cache[cache_key] = filtered\n                return filtered\n                \n        except Exception as e:\n            print(f\"🔍 Search error: {e}\")\n            return []\n\n    def index_results(self, results: List[Dict]):\n        \"\"\"Index both the snippets and full results\"\"\"\n        if not results:\n            raise ValueError(\"📭 No documents provided for indexing.\")\n            \n        self.raw_results = results\n        snippets = [r[\"snippet\"] for r in results if r.get(\"snippet\")]\n        \n        if not snippets:\n            raise ValueError(\"📭 No valid snippets found in results.\")\n            \n        self.documents = snippets\n        tokenized = [self._tokenize(doc) for doc in snippets]\n        self.bm25 = BM25Okapi(tokenized)\n        self.vector_db.build_index(snippets)\n\n    def hybrid_search(self, query: str, top_k: int = 5, alpha: float = 0.5) -> Optional[List[Dict]]:\n        \"\"\"Perform hybrid search on already indexed documents\"\"\"\n        if self.bm25 is None or not self.documents:\n            return None\n\n        # BM25 scoring\n        tokenized_query = self._tokenize(query)\n        bm25_scores = np.array(self.bm25.get_scores(tokenized_query))\n        \n        # Vector search scoring\n        vector_scores, indices = self.vector_db.search(query, top_k=len(self.documents))\n        if vector_scores is None:\n            return None\n            \n        # Normalize scores\n        bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-9)\n        vector_scores = (vector_scores - vector_scores.min()) / (vector_scores.max() - vector_scores.min() + 1e-9)\n        \n        # Combine scores\n        combined_scores = alpha * bm25_scores + (1 - alpha) * vector_scores\n        top_indices = np.argsort(combined_scores)[::-1][:top_k]\n        \n        # Return full results (not just snippets)\n        return [self.raw_results[i] for i in top_indices if i < len(self.raw_results)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:57:22.318926Z","iopub.execute_input":"2025-07-23T07:57:22.319434Z","iopub.status.idle":"2025-07-23T07:57:23.174373Z","shell.execute_reply.started":"2025-07-23T07:57:22.319408Z","shell.execute_reply":"2025-07-23T07:57:23.173570Z"}},"outputs":[{"name":"stdout","text":"Writing search_engine.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile vector_db.py\n\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nfrom typing import List, Optional\nimport os\nimport hashlib\nimport pickle\nfrom diskcache import Cache\n\nclass VectorDB:\n    def __init__(self, embedder):\n        self.embedder = embedder\n        self.index = None\n        self.embeddings = None\n        self.documents = []\n\n    def build_index(self, documents):\n        self.documents = documents\n        self.embeddings = self.embedder.encode(documents, convert_to_numpy=True, normalize_embeddings=True)\n        dim = self.embeddings.shape[1]\n\n        self.index = faiss.IndexFlatIP(dim)\n        self.index.add(self.embeddings)\n\n    def search(self, query, top_k=5):\n        query_vec = self.embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n        scores, indices = self.index.search(query_vec, top_k)\n        return scores[0], indices[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:57:23.175445Z","iopub.execute_input":"2025-07-23T07:57:23.175738Z","iopub.status.idle":"2025-07-23T07:57:23.186430Z","shell.execute_reply.started":"2025-07-23T07:57:23.175719Z","shell.execute_reply":"2025-07-23T07:57:23.185889Z"}},"outputs":[{"name":"stdout","text":"Writing vector_db.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile chat_engine.py\n\nfrom datetime import datetime\nfrom typing import List\nimport json\nimport torch\nfrom transformers import TextIteratorStreamer\nfrom threading import Thread\nimport re\n\nclass ChatEngine:\n    def __init__(self, model, tokenizer, device, hybrid_searcher=None):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.history = []\n        self.hybrid_searcher = hybrid_searcher\n        self.log_file = \"/kaggle/working/chat_logs.jsonl\"\n\n    def generate_response_stream(self, prompt: str):\n        streamer = TextIteratorStreamer(\n            self.tokenizer, \n            skip_prompt=True, \n            skip_special_tokens=True\n        )\n        inputs = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=768\n        ).to(self.device)\n        generation_kwargs = dict(\n            **inputs,\n            streamer=streamer,\n            max_new_tokens=400,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=self.tokenizer.eos_token_id,\n            num_beams=1,\n            use_cache=True\n        )\n        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n        thread.start()\n        return streamer\n\n    def generate_clean_response(self, query: str, context: List[str], raw_output: str) -> str:\n        cleaned = raw_output.strip()\n        for prefix in [\"You are an expert research assistant\", \"Context:\", \"Question:\", \"Answer:\"]:\n            if prefix.lower() in cleaned.lower():\n                cleaned = cleaned.split(prefix, 1)[-1].strip()\n\n        parts = re.split(r'(?i)\\n\\s*#+\\s*Sources|\\n\\s*(?:\\[?\\d+\\]?\\s*)?(https?://|Could you explain quantum computers|r/quantum on Reddit|From computing to quantum mechanics)', cleaned, 1)\n        main_content = parts[0].strip()\n        sources_raw = \"\"\n        if len(parts) > 1:\n            if parts[1] is None or not parts[1].strip():\n                sources_raw = parts[2].strip() if len(parts) > 2 else \"\"\n            else:\n                sources_raw = parts[1] + (parts[2] if len(parts) > 2 else \"\") \n                sources_raw = sources_raw.strip()\n\n        final_output_lines = []\n        final_output_lines.extend([line.strip() for line in main_content.splitlines() if line.strip()])\n\n        if sources_raw:\n            sources_raw = re.sub(r'\\n\\s*---\\s*\\n\\s*📚\\s*Sources\\s*\\n*', '\\n', sources_raw, flags=re.IGNORECASE).strip()\n            sources_raw = re.sub(r'\\[Comparison of LangChain[^\\]]*\\]\\s*\\\\n\\\\n---\\\\n\\\\n📚\\s*Sources\\s*\\\\n\\d*\\.\\s*', '', sources_raw, flags=re.IGNORECASE).strip()\n\n            final_output_lines.append(\"\\n## Sources\")\n\n            extracted_sources = []\n            for line in sources_raw.splitlines():\n                line = line.strip()\n                if not line:\n                    continue\n\n                url_match = re.search(r'(https?://[^\\s)]+)', line)\n                if url_match:\n                    url = url_match.group(1)\n                    title = line.split(url, 1)[0].strip()\n                    title = re.sub(r'\\s*Read more\\b', '', title, flags=re.IGNORECASE).strip()\n                    if not title:\n                        title = url\n                    extracted_sources.append(f\"[{title}]({url})\")\n                else:\n                    line = re.sub(r'^(?:\\[\\d+\\]|\\d+\\.)\\s*', '', line).strip()\n                    line = re.sub(r'\\s*Read more\\b', '', line, flags=re.IGNORECASE).strip()\n                    line = re.sub(r'(?i)could you explain quantum computers to a high school student|r/quantum on reddit|from computing to quantum mechanics|based on context', '', line).strip()\n                    if line:\n                        extracted_sources.append(line)\n\n            for i, source in enumerate(extracted_sources):\n                if source.strip():\n                    final_output_lines.append(f\"{i+1}. {source}\")\n\n        return \"\\n\".join(final_output_lines).strip()\n\n    def format_prompt(self, query: str, context: List[str]) -> str:\n        context_str = \"\\n\".join(f\"- {c.strip()}\" for c in context[:3]) if context else \"No context provided.\"\n\n        return f\"\"\"\nYou are an expert research assistant. Your task is to answer the user's question based on the provided context snippets.\n\n**Instructions:**\n1.  **Synthesize, do not copy:** Read all snippets and synthesize a comprehensive answer.\n2.  **Use Markdown:** Structure your answer with headings, bold text, and bullet points for clarity.\n3.  **Be Direct:** Start with a direct answer to the question.\n4.  **Professional Tone:** Maintain a sharp, factual, and professional tone.\n5.  **Conclusion:** After your main answer, provide a brief concluding paragraph under a \"## Conclusion\" heading.\n6.  **Sources:** If external sources (like URLs or article titles from the context) are relevant to the answer, list them clearly under a \"## Sources\" heading using a numbered Markdown list. For each source, provide a concise description and the full URL as a Markdown link. Example: `1. [LangChain Documentation](https://www.langchain.com/docs)` Ensure each source is on a new line.\n\n**Context Snippets:**\n{context_str}\n\n**Question:** {query}\n\n**Answer (in Markdown format):**\n\"\"\"\n\n    def search_and_format_prompt(self, query: str):\n        results = self.hybrid_searcher.search(query)\n        snippets = [r[\"snippet\"] for r in results if r.get(\"snippet\")]\n\n        if snippets:\n            self.hybrid_searcher.index(snippets)\n\n        top_snippets = self.hybrid_searcher.search(query, top_k=3)\n        prompt = self.format_prompt(query, top_snippets)\n        return prompt, top_snippets\n\n    def log_interaction(self, query: str, response: str, context: List[str]):\n        log = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"query\": query,\n            \"response\": response,\n            \"context\": [c[:300] for c in context]\n        }\n        with open(self.log_file, \"a\") as f:\n            f.write(json.dumps(log) + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:57:23.187210Z","iopub.execute_input":"2025-07-23T07:57:23.187457Z","iopub.status.idle":"2025-07-23T07:57:23.197685Z","shell.execute_reply.started":"2025-07-23T07:57:23.187433Z","shell.execute_reply":"2025-07-23T07:57:23.197146Z"}},"outputs":[{"name":"stdout","text":"Writing chat_engine.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile app.py\n\nfrom model_loader import load_model_and_tokenizer\nfrom search_engine import HybridSearchEngine\nfrom chat_engine import ChatEngine\nimport gradio as gr\nimport os\n\ndef initialize_system():\n    print(\"⚡ Initializing enhanced system...\")\n    model, tokenizer, device = load_model_and_tokenizer()\n    searcher = HybridSearchEngine()\n    return ChatEngine(model, tokenizer, device), searcher\n\ndef create_chat_fn(chat_engine, searcher):\n    def chat_fn(message, history):\n        # First perform web search\n        search_results = searcher.web_search(message)\n        context_snippets = []\n        \n        if search_results:\n            try:\n                # Index the results for hybrid search\n                searcher.index_results(search_results)\n                \n                # Get hybrid results\n                hybrid_results = searcher.hybrid_search(message)\n                if hybrid_results is not None:\n                    context_snippets = [r[\"snippet\"] for r in hybrid_results if r.get(\"snippet\")]\n            except ValueError as e:\n                print(f\"⚠️ Search indexing error: {e}\")\n                # Fallback to using raw search results\n                context_snippets = [r[\"snippet\"] for r in search_results if r.get(\"snippet\")]\n\n        prompt = chat_engine.format_prompt(message, context_snippets)\n        streamed_chunks = chat_engine.generate_response_stream(prompt)\n\n        final_response = \"\"\n        for chunk in streamed_chunks:\n            final_response += chunk\n            yield final_response\n\n        cleaned = chat_engine.generate_clean_response(message, context_snippets, final_response)\n        chat_engine.log_interaction(message, cleaned, context_snippets)\n        yield cleaned\n\n    return chat_fn\n\ndef launch_interface():\n    chat_engine, searcher = initialize_system()\n    demo = gr.ChatInterface(\n        fn=create_chat_fn(chat_engine, searcher),\n        title=\"🤖 SmolLM3-RAG Pro\",\n        description=\"AI assistant with premium-quality responses\",\n        examples=[\n            \"Explain quantum computing like I'm a high school student\",\n            \"What are Google DeepMind's most exciting current projects?\",\n            \"Break down how RAG systems work technically\"\n        ]\n    )\n    demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=True)\n\nif __name__ == \"__main__\":\n    launch_interface()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:57:23.199068Z","iopub.execute_input":"2025-07-23T07:57:23.199266Z","iopub.status.idle":"2025-07-23T07:57:23.212595Z","shell.execute_reply.started":"2025-07-23T07:57:23.199252Z","shell.execute_reply":"2025-07-23T07:57:23.212071Z"}},"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Now run the app\n!python app.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:57:23.213205Z","iopub.execute_input":"2025-07-23T07:57:23.213448Z","iopub.status.idle":"2025-07-23T08:27:19.630501Z","shell.execute_reply.started":"2025-07-23T07:57:23.213424Z","shell.execute_reply":"2025-07-23T08:27:19.629583Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n⚡ Initializing enhanced system...\nFetching 7 files:   0%|                                   | 0/7 [00:00<?, ?it/s]\ntokenizer_config.json:   0%|                        | 0.00/50.4k [00:00<?, ?B/s]\u001b[A\n\n\nadapter_config.json:   0%|                            | 0.00/855 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n\nadapter_config.json: 100%|█████████████████████| 855/855 [00:00<00:00, 1.56MB/s]\u001b[A\u001b[A\n\n\n\n\nREADME.md: 100%|███████████████████████████| 5.17k/5.17k [00:00<00:00, 5.32MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\ntokenizer_config.json: 100%|███████████████| 50.4k/50.4k [00:00<00:00, 13.0MB/s]\nchat_template.jinja: 100%|█████████████████| 5.60k/5.60k [00:00<00:00, 4.53MB/s]\n\n.gitattributes: 100%|██████████████████████| 1.57k/1.57k [00:00<00:00, 14.4MB/s]\u001b[A\nFetching 7 files:  14%|███▊                       | 1/7 [00:00<00:01,  5.55it/s]\nspecial_tokens_map.json: 100%|█████████████████| 175/175 [00:00<00:00, 1.97MB/s]\u001b[A\n\ntokenizer.json:   0%|                               | 0.00/17.2M [00:00<?, ?B/s]\u001b[A\ntokenizer.json:   2%|▍                       | 294k/17.2M [00:00<00:34, 489kB/s]\u001b[A\ntokenizer.json: 100%|██████████████████████| 17.2M/17.2M [00:00<00:00, 23.2MB/s]\u001b[A\nFetching 7 files: 100%|███████████████████████████| 7/7 [00:00<00:00,  7.16it/s]\nconfig.json: 1.88kB [00:00, 8.32MB/s]\n2025-07-23 07:57:39.423475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753257459.584329     176 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753257459.631188     176 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nmodel.safetensors.index.json: 26.9kB [00:00, 89.9MB/s]\nFetching 2 files:   0%|                                   | 0/2 [00:00<?, ?it/s]\nmodel-00001-of-00002.safetensors:   0%|             | 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:   0%|             | 0.00/1.18G [00:00<?, ?B/s]\u001b[A\u001b[A\n\nmodel-00002-of-00002.safetensors:   0%|      | 552k/1.18G [00:00<26:28, 745kB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:   0%|    | 776k/4.97G [00:00<1:38:46, 838kB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   0%|    | 2.79M/4.97G [00:01<46:04, 1.80MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   0%|    | 4.40M/4.97G [00:01<31:28, 2.63MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:   6%|▏   | 67.7M/1.18G [00:02<00:32, 34.2MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00002.safetensors:  12%|▌    | 143M/1.18G [00:02<00:17, 60.7MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00002.safetensors:  15%|▊    | 180M/1.18G [00:02<00:12, 81.3MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:   0%|    | 12.7M/4.97G [00:03<16:06, 5.13MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  18%|▉    | 207M/1.18G [00:03<00:16, 59.1MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:   0%|    | 18.6M/4.97G [00:03<12:49, 6.43MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|    | 25.6M/4.97G [00:07<26:13, 3.14MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  19%|▉    | 224M/1.18G [00:09<01:13, 13.1MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:   1%|    | 44.7M/4.97G [00:10<16:50, 4.87MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  23%|█▏   | 269M/1.18G [00:10<00:48, 19.0MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:   2%|     | 112M/4.97G [00:10<04:32, 17.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏    | 127M/4.97G [00:10<03:47, 21.3MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  26%|█▎   | 312M/1.18G [00:11<00:33, 25.7MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏    | 186M/4.97G [00:13<03:24, 23.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▏    | 231M/4.97G [00:13<02:18, 34.1MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  32%|█▌   | 379M/1.18G [00:13<00:31, 25.6MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▎    | 298M/4.97G [00:14<01:36, 48.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▎    | 318M/4.97G [00:14<01:26, 53.9MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  34%|█▋   | 407M/1.18G [00:14<00:26, 28.8MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍    | 385M/4.97G [00:14<00:57, 79.1MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▍    | 452M/4.97G [00:15<01:04, 69.9MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▌    | 519M/4.97G [00:16<00:51, 85.7MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  40%|██   | 474M/1.18G [00:16<00:22, 31.3MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▋     | 586M/4.97G [00:16<00:37, 116MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▊     | 653M/4.97G [00:16<00:28, 151MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▊     | 720M/4.97G [00:16<00:29, 144MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  46%|██▎  | 542M/1.18G [00:17<00:15, 40.4MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▉     | 787M/4.97G [00:17<00:24, 168MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|█     | 835M/4.97G [00:17<00:27, 149MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  51%|██▌  | 609M/1.18G [00:17<00:11, 48.0MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  18%|█     | 902M/4.97G [00:18<00:26, 155MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|▉    | 941M/4.97G [00:19<00:57, 69.7MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|▊   | 1.01G/4.97G [00:19<00:40, 98.2MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  57%|██▊  | 676M/1.18G [00:20<00:12, 41.1MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  22%|█    | 1.08G/4.97G [00:20<00:35, 109MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|█▏   | 1.13G/4.97G [00:20<00:29, 130MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 1.19G/4.97G [00:20<00:22, 168MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█▎   | 1.26G/4.97G [00:21<00:20, 184MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█▎   | 1.33G/4.97G [00:21<00:18, 196MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  63%|███▏ | 743M/1.18G [00:21<00:10, 44.1MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█▍   | 1.39G/4.97G [00:21<00:15, 230MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▍   | 1.43G/4.97G [00:21<00:15, 224MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▌   | 1.50G/4.97G [00:21<00:12, 284MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  32%|█▌   | 1.57G/4.97G [00:22<00:11, 283MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  72%|███▌ | 855M/1.18G [00:22<00:05, 60.4MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00002.safetensors:  83%|████▏| 983M/1.18G [00:22<00:02, 95.3MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▋   | 1.70G/4.97G [00:22<00:13, 245MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▊   | 1.77G/4.97G [00:22<00:12, 250MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▊   | 1.83G/4.97G [00:23<00:12, 241MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|█▉   | 1.97G/4.97G [00:23<00:09, 318MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|██   | 2.10G/4.97G [00:23<00:07, 373MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|██▏  | 2.17G/4.97G [00:24<00:09, 306MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  89%|███▌| 1.05G/1.18G [00:24<00:01, 69.2MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  46%|██▎  | 2.30G/4.97G [00:24<00:10, 248MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  49%|██▍  | 2.44G/4.97G [00:25<00:08, 312MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  50%|██▌  | 2.51G/4.97G [00:25<00:09, 254MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  52%|██▌  | 2.57G/4.97G [00:25<00:08, 293MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██▋  | 2.64G/4.97G [00:25<00:07, 294MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  54%|██▋  | 2.71G/4.97G [00:26<00:11, 196MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  57%|██▊  | 2.84G/4.97G [00:26<00:07, 300MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▉  | 2.90G/4.97G [00:27<00:08, 252MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  60%|██▉  | 2.97G/4.97G [00:28<00:16, 124MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  61%|███  | 3.04G/4.97G [00:28<00:12, 152MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|███▏ | 3.10G/4.97G [00:29<00:17, 109MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|███▏ | 3.17G/4.97G [00:30<00:15, 112MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors:  94%|███▊| 1.12G/1.18G [00:30<00:02, 30.5MB/s]\u001b[A\u001b[A\nmodel-00001-of-00002.safetensors:  65%|███▎ | 3.24G/4.97G [00:30<00:11, 147MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  67%|███▎ | 3.31G/4.97G [00:30<00:10, 154MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|███▍ | 3.37G/4.97G [00:31<00:11, 142MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  69%|███▍ | 3.44G/4.97G [00:31<00:09, 162MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|███▌ | 3.51G/4.97G [00:31<00:07, 201MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  72%|███▌ | 3.57G/4.97G [00:32<00:10, 129MB/s]\u001b[A\n\nmodel-00002-of-00002.safetensors: 100%|████| 1.18G/1.18G [00:33<00:00, 35.2MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00002.safetensors:  73%|██▉ | 3.64G/4.97G [00:34<00:15, 84.4MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  76%|███▊ | 3.78G/4.97G [00:34<00:09, 120MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███▊ | 3.83G/4.97G [00:34<00:08, 135MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  78%|███▉ | 3.89G/4.97G [00:35<00:06, 155MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  80%|███▉ | 3.96G/4.97G [00:35<00:05, 184MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  81%|████ | 4.03G/4.97G [00:35<00:05, 181MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  82%|████ | 4.09G/4.97G [00:35<00:04, 216MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  84%|████▏| 4.16G/4.97G [00:36<00:03, 228MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  85%|████▎| 4.23G/4.97G [00:36<00:02, 250MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  86%|████▎| 4.30G/4.97G [00:36<00:03, 192MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|████▍| 4.36G/4.97G [00:37<00:03, 195MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  89%|████▍| 4.43G/4.97G [00:37<00:03, 172MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  91%|████▌| 4.50G/4.97G [00:38<00:03, 137MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  92%|████▌| 4.56G/4.97G [00:39<00:03, 105MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  93%|████▋| 4.63G/4.97G [00:39<00:03, 107MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  95%|████▋| 4.70G/4.97G [00:40<00:02, 100MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  96%|████▊| 4.77G/4.97G [00:41<00:01, 101MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  97%|███▉| 4.83G/4.97G [00:42<00:01, 91.8MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  99%|████▉| 4.90G/4.97G [00:42<00:00, 112MB/s]\u001b[A\nmodel-00001-of-00002.safetensors: 100%|█████| 4.97G/4.97G [00:42<00:00, 116MB/s]\u001b[A\nFetching 2 files: 100%|███████████████████████████| 2/2 [00:42<00:00, 21.43s/it]\nLoading checkpoint shards: 100%|██████████████████| 2/2 [00:11<00:00,  5.56s/it]\ngeneration_config.json: 100%|██████████████████| 182/182 [00:00<00:00, 1.19MB/s]\nadapter_model.safetensors: 100%|███████████| 60.5M/60.5M [00:00<00:00, 66.3MB/s]\nmodules.json: 100%|████████████████████████████| 349/349 [00:00<00:00, 1.22MB/s]\nconfig_sentence_transformers.json: 100%|████████| 116/116 [00:00<00:00, 712kB/s]\nREADME.md: 10.5kB [00:00, 21.9MB/s]\nsentence_bert_config.json: 100%|██████████████| 53.0/53.0 [00:00<00:00, 265kB/s]\nconfig.json: 100%|█████████████████████████████| 612/612 [00:00<00:00, 2.98MB/s]\nmodel.safetensors: 100%|███████████████████| 90.9M/90.9M [00:01<00:00, 90.4MB/s]\ntokenizer_config.json: 100%|███████████████████| 350/350 [00:00<00:00, 1.08MB/s]\nvocab.txt: 232kB [00:00, 13.8MB/s]\ntokenizer.json: 466kB [00:00, 24.6MB/s]\nspecial_tokens_map.json: 100%|██████████████████| 112/112 [00:00<00:00, 951kB/s]\nconfig.json: 100%|█████████████████████████████| 190/190 [00:00<00:00, 1.57MB/s]\n/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n  self.chatbot = Chatbot(\n* Running on local URL:  http://0.0.0.0:7860\n* Running on public URL: https://b9e854a1e6aa0703be.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\nBatches: 100%|████████████████████████████████████| 1/1 [00:00<00:00,  2.29it/s]\nBatches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 97.36it/s]\nBatches: 100%|███████████████████████████████████| 1/1 [00:00<00:00, 121.68it/s]\nBatches: 100%|███████████████████████████████████| 1/1 [00:00<00:00, 212.10it/s]\nBatches: 100%|███████████████████████████████████| 1/1 [00:00<00:00, 113.76it/s]\nBatches: 100%|███████████████████████████████████| 1/1 [00:00<00:00, 225.68it/s]\n^C\nKeyboard interruption in main thread... closing server.\nKilling tunnel 0.0.0.0:7860 <> https://b9e854a1e6aa0703be.gradio.live\nProcess ForkProcess-1:\nProcess ForkProcess-4:\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 102, in get\n    with self._rlock:\n  File \"/usr/lib/python3.11/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n    call_item = call_queue.get(block=True)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n    res = self._recv_bytes()\n          ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n    buf = self._recv(4)\n          ^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n    chunk = read(handle, remaining)\n            ^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":7}]}