{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b1584beee2d940e7a28af47326216eb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2586e670e2064c4088b8292c163f3fd8","IPY_MODEL_ed610d2c04a8491eae730a7d934376bc","IPY_MODEL_cddd38ab17e7434c889bfee5a07c3066"],"layout":"IPY_MODEL_aaf15ec2deac4d9d8036e12e0c6efbb6"}},"2586e670e2064c4088b8292c163f3fd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_738fd91e4ec546e3b5c4307ee4b15a25","placeholder":"​","style":"IPY_MODEL_dcea76d82d0a414e9d531f0c985d4a51","value":"Loading checkpoint shards: 100%"}},"ed610d2c04a8491eae730a7d934376bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46822ca36b03484f8dbb299b0bd29047","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_10075daabc0f436385b8f901973ad1bf","value":2}},"cddd38ab17e7434c889bfee5a07c3066":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d98a9e5631134d1ca79f4a088fb1d2fe","placeholder":"​","style":"IPY_MODEL_9909b105647643ce99a3dca470126164","value":" 2/2 [00:25&lt;00:00, 11.60s/it]"}},"aaf15ec2deac4d9d8036e12e0c6efbb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"738fd91e4ec546e3b5c4307ee4b15a25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcea76d82d0a414e9d531f0c985d4a51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46822ca36b03484f8dbb299b0bd29047":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10075daabc0f436385b8f901973ad1bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d98a9e5631134d1ca79f4a088fb1d2fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9909b105647643ce99a3dca470126164":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac6a6a27860e4fa29ccb80391c7281a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b1a2ede69d754cdd818afced04e047f5","IPY_MODEL_dc087ec9d297449a8db488f9e4c2be0f","IPY_MODEL_180f379056dd46ada784b069361d3e56"],"layout":"IPY_MODEL_cc87965dcec247589bce534aa9a4ec58"}},"b1a2ede69d754cdd818afced04e047f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08a18e95924c4f668ad70d5a0f48e872","placeholder":"​","style":"IPY_MODEL_10c6c2be11c142abba0b35f568d629d5","value":"Adding EOS to train dataset: 100%"}},"dc087ec9d297449a8db488f9e4c2be0f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3402d54c71e14fd98de4876a9ff9e0d1","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8163945228c74b32b4ac4c218f100a60","value":1000}},"180f379056dd46ada784b069361d3e56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_265f3c12d14d4d9d9a059f4dd667f9cd","placeholder":"​","style":"IPY_MODEL_2a2f428e387043c4925b70368ce15ee8","value":" 1000/1000 [00:00&lt;00:00, 9833.67 examples/s]"}},"cc87965dcec247589bce534aa9a4ec58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08a18e95924c4f668ad70d5a0f48e872":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10c6c2be11c142abba0b35f568d629d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3402d54c71e14fd98de4876a9ff9e0d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8163945228c74b32b4ac4c218f100a60":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"265f3c12d14d4d9d9a059f4dd667f9cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a2f428e387043c4925b70368ce15ee8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dba476834bd748f4aed7d728f7b759bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_607b6687ff2d49f0a7140f94272633c1","IPY_MODEL_f537b006a1224dc89fad7e72ef297d28","IPY_MODEL_423d4281cc834105ad2f45c173e9274d"],"layout":"IPY_MODEL_73f3a5be96084773ba00a2812599ad77"}},"607b6687ff2d49f0a7140f94272633c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bb6759853eb4553bc8b8c67e446ddc7","placeholder":"​","style":"IPY_MODEL_7d754603fc8249f0b56e30584a2a917f","value":"Tokenizing train dataset: 100%"}},"f537b006a1224dc89fad7e72ef297d28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b37a45940b914c9c8733961720f7cb90","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ca718a74a024d879bccaea944bc3ffb","value":1000}},"423d4281cc834105ad2f45c173e9274d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14fad5e48ab74dd0a60369edfb21d8e5","placeholder":"​","style":"IPY_MODEL_f45882a6628d4f3d9c577bc1f9676cf2","value":" 1000/1000 [00:01&lt;00:00, 743.72 examples/s]"}},"73f3a5be96084773ba00a2812599ad77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bb6759853eb4553bc8b8c67e446ddc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d754603fc8249f0b56e30584a2a917f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b37a45940b914c9c8733961720f7cb90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ca718a74a024d879bccaea944bc3ffb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14fad5e48ab74dd0a60369edfb21d8e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f45882a6628d4f3d9c577bc1f9676cf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d6acdd5112645ff8f6275d658527f41":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6bf8b486ef0447b95bc0793d3411f14","IPY_MODEL_829ccde7455c4aa7bdbd9839cecb6817","IPY_MODEL_2c8e6f7c4d434782be2e713ba835f9d9"],"layout":"IPY_MODEL_9011dacc9d2e409aa43083fcabfb7d98"}},"a6bf8b486ef0447b95bc0793d3411f14":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_faf9b3ac846644df8de044cb574ac986","placeholder":"​","style":"IPY_MODEL_2f0777cc745d4e42b7f2fb8e53a187f7","value":"Truncating train dataset: 100%"}},"829ccde7455c4aa7bdbd9839cecb6817":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e294475bdbe49688a8dad2c4146652d","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78f6c31d1a0f4f619b7c0c29d290d717","value":1000}},"2c8e6f7c4d434782be2e713ba835f9d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33888beee24a44afa08d20aefef61c01","placeholder":"​","style":"IPY_MODEL_2662786c46c247fc89cdb3025b357ea2","value":" 1000/1000 [00:00&lt;00:00, 28384.77 examples/s]"}},"9011dacc9d2e409aa43083fcabfb7d98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"faf9b3ac846644df8de044cb574ac986":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f0777cc745d4e42b7f2fb8e53a187f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e294475bdbe49688a8dad2c4146652d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78f6c31d1a0f4f619b7c0c29d290d717":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33888beee24a44afa08d20aefef61c01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2662786c46c247fc89cdb3025b357ea2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"print(\"🔧 Installing packages (Colab-safe versions)...\")\nimport subprocess\nimport sys\nimport os\n\n# First, ensure we have clean installations\npackages_to_uninstall = [\"bitsandbytes\", \"triton\"]\nfor pkg in packages_to_uninstall:\n    try:\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", pkg, \"-y\"],\n                      capture_output=True, check=False)\n    except:\n        pass\n\n# Install core packages without problematic dependencies\ncore_packages = [\n    \"transformers>=4.47.0\",\n    \"torch\",\n    \"datasets\",\n    \"peft>=0.8.0\",\n    \"trl>=0.7.0\",\n    \"accelerate\",\n    \"huggingface_hub\",\n]\n\nfor package in core_packages:\n    try:\n        result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package],\n                              capture_output=True, text=True, timeout=300)\n        if result.returncode == 0:\n            print(f\"✅ {package} installed\")\n        else:\n            print(f\"⚠️ {package} had issues: {result.stderr}\")\n    except Exception as e:\n        print(f\"⚠️ {package} installation failed: {e}\")\n\n# Import with error handling\nprint(\"📦 Importing libraries...\")\ntry:\n    import torch\n    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n    from datasets import Dataset\n    from peft import LoraConfig, get_peft_model\n    from huggingface_hub import login\n    print(\"✅ Core imports successful\")\nexcept ImportError as e:\n    print(f\"❌ Import error: {e}\")\n    print(\"🔄 Trying alternative imports...\")\n\n# Try importing TRL with fallbacks\ntry:\n    from trl import SFTTrainer\n    TRL_AVAILABLE = True\n    print(\"✅ TRL SFTTrainer available\")\nexcept ImportError:\n    print(\"⚠️ TRL SFTTrainer not available, will use alternative\")\n    TRL_AVAILABLE = False\n\nprint(f\"🖥️ PyTorch: {torch.__version__}\")\nprint(f\"🚀 CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"💾 GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# Set device and memory management\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# Login to Hugging Face\nHF_TOKEN = \"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\"\nos.environ[\"HF_TOKEN\"] = HF_TOKEN\nlogin(token=HF_TOKEN)\nprint(\"✅ Logged in to Hugging Face\")\n\n# Model loading with better error handling\nmodel_name = \"HuggingFaceTB/SmolLM3-3B\"\nprint(f\"🔄 Loading {model_name}...\")\n\ntry:\n    # Load tokenizer first\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        use_fast=True\n    )\n\n    # Fix tokenizer - CRITICAL: Use a different pad token\n    if tokenizer.pad_token is None:\n        # Use a special token for padding instead of EOS\n        tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n\n    print(\"✅ Tokenizer loaded\")\n\n    # Load model with conservative settings\n    print(\"📦 Loading model (this may take 5-10 minutes)...\")\n\n    try:\n        # First attempt: Load normally\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n            device_map=None,  # Don't use device_map to avoid meta tensor issues\n            use_cache=False  # Disable caching for training\n        )\n\n        # Manually move to device after loading\n        if torch.cuda.is_available():\n            model = model.to(device)\n\n    except Exception as e:\n        print(f\"⚠️ Standard loading failed: {e}\")\n        print(\"🔄 Trying alternative loading method...\")\n\n        # Alternative: Load without low_cpu_mem_usage\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float32,  # Use float32 for compatibility\n            trust_remote_code=True,\n            low_cpu_mem_usage=False,\n            use_cache=False\n        )\n\n        # Move to device and convert to appropriate dtype\n        if torch.cuda.is_available():\n            model = model.to(device)\n            if torch.cuda.get_device_capability()[0] >= 7:  # Check if supports fp16\n                model = model.half()\n\n    # Resize token embeddings if we added new tokens\n    if len(tokenizer) > model.config.vocab_size:\n        model.resize_token_embeddings(len(tokenizer))\n\n    # Ensure model is on the correct device after resizing\n    if torch.cuda.is_available():\n        model = model.to(device)\n\n    print(\"✅ Model loaded successfully\")\n    print(f\"📊 Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n    print(f\"🎯 Model device: {next(model.parameters()).device}\")\n    print(f\"📊 Model dtype: {next(model.parameters()).dtype}\")\n\nexcept Exception as e:\n    print(f\"❌ Model loading failed: {e}\")\n    print(\"🔄 Trying with smaller fallback model...\")\n\n    # Fallback to smaller model\n    model_name = \"HuggingFaceTB/SmolLM2-1.7B\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    if tokenizer.pad_token is None:\n        tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float32,\n        trust_remote_code=True,\n        low_cpu_mem_usage=False,\n        use_cache=False\n    )\n\n    if len(tokenizer) > model.config.vocab_size:\n        model.resize_token_embeddings(len(tokenizer))\n\n    if torch.cuda.is_available():\n        model = model.to(device)\n\n# Create training dataset with proper formatting\nprint(\"📚 Preparing training data...\")\n\n# Better training examples with clear conversation boundaries\ntraining_conversations = [\n    # Greetings and introductions\n    [{\"role\": \"user\", \"content\": \"Hello\"},\n     {\"role\": \"assistant\", \"content\": \"Hello! How can I help you today?\"}],\n\n    [{\"role\": \"user\", \"content\": \"Hi there\"},\n     {\"role\": \"assistant\", \"content\": \"Hi! I'm here to assist you. What would you like to know?\"}],\n\n    [{\"role\": \"user\", \"content\": \"Good morning\"},\n     {\"role\": \"assistant\", \"content\": \"Good morning! I hope you're having a great day. How can I help?\"}],\n\n    # Questions about capabilities\n    [{\"role\": \"user\", \"content\": \"What can you do?\"},\n     {\"role\": \"assistant\", \"content\": \"I can help with questions, provide explanations, assist with writing, and have conversations on various topics. What would you like help with?\"}],\n\n    [{\"role\": \"user\", \"content\": \"Tell me about yourself\"},\n     {\"role\": \"assistant\", \"content\": \"I'm an AI assistant designed to be helpful, informative, and engaging. I enjoy helping people with their questions and tasks.\"}],\n\n    # Helpful responses\n    [{\"role\": \"user\", \"content\": \"I need help with something\"},\n     {\"role\": \"assistant\", \"content\": \"I'd be happy to help! Could you tell me more about what you need assistance with?\"}],\n\n    [{\"role\": \"user\", \"content\": \"Can you explain this?\"},\n     {\"role\": \"assistant\", \"content\": \"Of course! I'd be glad to explain. What specifically would you like me to clarify?\"}],\n\n    # Polite interactions\n    [{\"role\": \"user\", \"content\": \"Thank you\"},\n     {\"role\": \"assistant\", \"content\": \"You're very welcome! I'm glad I could help. Is there anything else I can assist you with?\"}],\n\n    [{\"role\": \"user\", \"content\": \"Thanks for your help\"},\n     {\"role\": \"assistant\", \"content\": \"My pleasure! I'm always here if you need more assistance.\"}],\n\n    # Knowledge queries\n    [{\"role\": \"user\", \"content\": \"Tell me a fun fact\"},\n     {\"role\": \"assistant\", \"content\": \"Here's a fun fact: Octopuses have three hearts and blue blood! Two hearts pump blood to their gills, while the third pumps blood to the rest of their body.\"}],\n\n    [{\"role\": \"user\", \"content\": \"How are you?\"},\n     {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you for asking! I'm ready to help you with whatever you need.\"}],\n]\n\n# Create expanded dataset with better formatting\ndef create_training_text(conversation):\n    \"\"\"Create properly formatted training text\"\"\"\n    text = \"\"\n    for message in conversation:\n        role = message[\"role\"]\n        content = message[\"content\"]\n        if role == \"user\":\n            text += f\"User: {content}\\n\"\n        else:\n            text += f\"Assistant: {content}\\n\"\n    text += \"<|endoftext|>\"  # Clear end marker\n    return text\n\n# Create training data\ntraining_data = []\nfor conversation in training_conversations:\n    training_data.append({\n        \"text\": create_training_text(conversation)\n    })\n\n# Multiply dataset for more training examples\ntraining_data = training_data * 20  # 220 examples total\n\ndataset = Dataset.from_list(training_data)\n\n# Filter out any problematic samples\ndataset = dataset.filter(lambda x: len(x[\"text\"]) > 10 and len(x[\"text\"]) < 1000)\n\nprint(f\"✅ Dataset ready: {len(dataset)} training samples\")\nprint(f\"📝 Sample text: {dataset[0]['text'][:100]}...\")\n\n# Configure LoRA for memory efficiency\nprint(\"⚙️ Setting up LoRA...\")\n\n# More conservative LoRA config\nlora_config = LoraConfig(\n    r=4,  # Smaller rank for stability\n    lora_alpha=8,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Standard attention modules\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"📊 Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n\n# Training configuration with better settings\nprint(\"🏋️ Configuring training...\")\n\ntraining_args = TrainingArguments(\n    output_dir=\"./smollm3_fixed\",\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    learning_rate=5e-5,  # Lower learning rate for stability\n    num_train_epochs=2,\n    max_steps=50,  # Fewer steps for testing\n    warmup_steps=5,\n    logging_steps=5,\n    save_strategy=\"steps\",\n    save_steps=25,\n    optim=\"adamw_torch\",\n    fp16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7,\n    bf16=False,\n    gradient_checkpointing=False,  # Disable to avoid caching issues\n    dataloader_pin_memory=False,\n    remove_unused_columns=False,\n    report_to=\"none\",\n    push_to_hub=False,\n    lr_scheduler_type=\"cosine\",\n    weight_decay=0.01,\n)\n\n# Create trainer with improved error handling\nif TRL_AVAILABLE:\n    print(\"🚀 Using SFTTrainer...\")\n    try:\n        from trl import SFTTrainer\n\n        # Custom formatting function for better control\n        def formatting_prompts_func(examples):\n            texts = []\n            for text in examples[\"text\"]:\n                # Ensure text ends properly\n                if not text.endswith(\"<|endoftext|>\"):\n                    text += \"<|endoftext|>\"\n                texts.append(text)\n            return texts\n\n        trainer = SFTTrainer(\n            model=model,\n            train_dataset=dataset,\n            args=training_args,\n            tokenizer=tokenizer,\n            formatting_func=formatting_prompts_func,\n            max_seq_length=128,  # Shorter sequences\n            packing=False,\n            dataset_num_proc=1,  # Single process to avoid issues\n        )\n        print(\"✅ SFTTrainer initialized\")\n    except Exception as e:\n        print(f\"⚠️ SFTTrainer failed: {e}\")\n        TRL_AVAILABLE = False\n\nif not TRL_AVAILABLE:\n    print(\"🔄 Using standard Trainer...\")\n    from transformers import Trainer, DataCollatorForLanguageModeling\n\n    # Tokenize dataset properly - fix the batching issue\n    def tokenize_function(examples):\n        # Process each text individually to avoid tensor shape issues\n        tokenized_inputs = []\n        tokenized_labels = []\n\n        for text in examples[\"text\"]:\n            # Tokenize individual text\n            tokens = tokenizer(\n                text,\n                truncation=True,\n                max_length=128,\n                padding=False,\n                add_special_tokens=True\n            )\n            tokenized_inputs.append(tokens[\"input_ids\"])\n            tokenized_labels.append(tokens[\"input_ids\"].copy())  # Labels same as input for CLM\n\n        return {\n            \"input_ids\": tokenized_inputs,\n            \"labels\": tokenized_labels\n        }\n\n    tokenized_dataset = dataset.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=dataset.column_names,\n        batch_size=1000  # Process in smaller batches\n    )\n\n    # Custom data collator that handles variable lengths properly\n    def custom_data_collator(features):\n        # Extract input_ids and labels\n        input_ids = [f[\"input_ids\"] for f in features]\n        labels = [f[\"labels\"] for f in features]\n\n        # Pad sequences to the same length\n        max_length = max(len(ids) for ids in input_ids)\n        max_length = min(max_length, 128)  # Cap at 128\n\n        padded_input_ids = []\n        padded_labels = []\n        attention_masks = []\n\n        for ids, lbls in zip(input_ids, labels):\n            # Truncate if too long\n            if len(ids) > max_length:\n                ids = ids[:max_length]\n                lbls = lbls[:max_length]\n\n            # Create attention mask\n            attention_mask = [1] * len(ids)\n\n            # Pad sequences\n            pad_length = max_length - len(ids)\n            if pad_length > 0:\n                ids.extend([tokenizer.pad_token_id] * pad_length)\n                lbls.extend([-100] * pad_length)  # -100 is ignored in loss computation\n                attention_mask.extend([0] * pad_length)\n\n            padded_input_ids.append(ids)\n            padded_labels.append(lbls)\n            attention_masks.append(attention_mask)\n\n        return {\n            \"input_ids\": torch.tensor(padded_input_ids, dtype=torch.long),\n            \"labels\": torch.tensor(padded_labels, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attention_masks, dtype=torch.long)\n        }\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n        data_collator=custom_data_collator,\n    )\n\n# Start training with better error handling\nprint(\"🚀 Starting training...\")\ntry:\n    # Clear any cached states\n    if hasattr(model, 'gradient_checkpointing_enable'):\n        model.gradient_checkpointing_disable()\n\n    trainer.train()\n    print(\"✅ Training completed successfully!\")\n\nexcept Exception as e:\n    print(f\"❌ Training failed: {e}\")\n    print(\"💡 This might be due to memory issues or model compatibility\")\n    import traceback\n    traceback.print_exc()\n\n# Save model\nprint(\"💾 Saving model...\")\ntry:\n    trainer.save_model()\n    tokenizer.save_pretrained(\"./smollm3_fixed\")\n\n    # Upload to HuggingFace\n    model.push_to_hub(\n        \"soupstick/smollm3-fixed\",\n        private=True,\n        use_auth_token=HF_TOKEN\n    )\n    tokenizer.push_to_hub(\n        \"soupstick/smollm3-fixed\",\n        private=True,\n        use_auth_token=HF_TOKEN\n    )\n    print(\"✅ Model saved and uploaded!\")\n\nexcept Exception as e:\n    print(f\"⚠️ Upload error: {e}\")\n    print(\"Model saved locally in './smollm3_fixed'\")\n\n# Test the model with better generation settings\nprint(\"🧪 Testing the fine-tuned model...\")\ntry:\n    # Ensure model is in eval mode\n    model.eval()\n\n    test_prompts = [\n        \"User: Hello\\nAssistant:\",\n        \"User: How are you?\\nAssistant:\",\n        \"User: What can you do?\\nAssistant:\"\n    ]\n\n    for test_prompt in test_prompts:\n        print(f\"\\n🔤 Testing: {test_prompt}\")\n\n        inputs = tokenizer.encode(test_prompt, return_tensors=\"pt\")\n        if torch.cuda.is_available():\n            inputs = inputs.to(device)\n\n        with torch.no_grad():\n            outputs = model.generate(\n                inputs,\n                max_new_tokens=30,\n                do_sample=True,\n                temperature=0.8,\n                top_p=0.9,\n                top_k=50,\n                repetition_penalty=1.1,  # Prevent repetition\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n                use_cache=True  # Enable cache for generation\n            )\n\n        # Decode response\n        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        new_text = full_response[len(test_prompt):].strip()\n\n        print(f\"🤖 Response: {new_text}\")\n\nexcept Exception as e:\n    print(f\"⚠️ Testing failed: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\n🎉 Fixed fine-tuning completed!\")\nprint(f\"📍 Model: {model_name}\")\nprint(f\"🔗 HuggingFace: soupstick/smollm3-fixed\")\nprint(\"🚀 Ready for your RAG chatbot!\")\n\n# Clean up memory\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\nimport gc\ngc.collect()\nprint(\"🧹 Memory cleaned up\")","metadata":{"id":"00JGtkPhyCto","jupyter":{"source_hidden":true},"_kg_hide-input":true}},{"cell_type":"markdown","source":"# Fine Tuning V2","metadata":{"id":"AeJcGI-Gyr0_"}},{"cell_type":"code","source":"print(\"🔧 Installing packages (Colab-safe versions)...\")\nimport subprocess\nimport sys\nimport os\n\n# Uninstall problematic packages cleanly if present\npackages_to_uninstall = [\"triton\"]\nfor pkg in packages_to_uninstall:\n    try:\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", pkg, \"-y\"],\n                       capture_output=True, check=False)\n    except Exception:\n        pass\n\n# Core packages with specific version constraints for compatibility\ncore_packages = [\n    \"torch>=2.1.0\",                         # stable torch version with CUDA support\n    \"transformers>=4.53.0\",                 # supports newer LLMs like phi-3, smolLM\n    \"datasets>=2.18.0\",                     # stable for Dataset()\n    \"peft>=0.8.0\",                          # LoRA and parameter-efficient tuning\n    \"trl>=0.7.0\",                           # for SFTTrainer\n    \"accelerate>=0.25.0\",                   # required for trainer\n    \"huggingface_hub>=0.21.4\",             # for model upload\n    \"feedparser>=6.0.11\",                   # for RSS parsing\n    \"ddgs>=1.0.5\",                          # updated DuckDuckGo search wrapper\n    \"requests\",                             # used in RSS/HTTP fetches\n    \"bitsandbytes>=0.41.3\"\n]\n\nfor package in core_packages:\n    try:\n        result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package],\n                                capture_output=True, text=True, timeout=300)\n        if result.returncode == 0:\n            print(f\"✅ {package} installed\")\n        else:\n            print(f\"⚠️ {package} had issues: {result.stderr}\")\n    except Exception as e:\n        print(f\"❌ {package} installation failed: {e}\")\n\n# Import core libraries\nprint(\"📦 Importing libraries...\")\ntry:\n    import torch\n    import requests\n    import feedparser\n    from ddgs import DDGS\n    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n    from datasets import Dataset\n    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n    from huggingface_hub import login\n    print(\"✅ Core imports successful\")\nexcept ImportError as e:\n    print(f\"❌ Import error: {e}\")\n\n# Import TRL trainer with fallback flag\ntry:\n    from trl import SFTTrainer\n    TRL_AVAILABLE = True\n    print(\"✅ TRL SFTTrainer available\")\nexcept ImportError:\n    print(\"⚠️ TRL SFTTrainer not available, will use alternative\")\n    TRL_AVAILABLE = False\n\n# Environment info\nprint(f\"🖥️ PyTorch: {torch.__version__}\")\nprint(f\"🚀 CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"💾 GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jsnoIAILtjYp","outputId":"6aa11b21-5863-43c1-bdd5-be38a2626bd7","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T12:02:48.675458Z","iopub.execute_input":"2025-07-21T12:02:48.676014Z","iopub.status.idle":"2025-07-21T12:05:59.877534Z","shell.execute_reply.started":"2025-07-21T12:02:48.675963Z","shell.execute_reply":"2025-07-21T12:05:59.876908Z"}},"outputs":[{"name":"stdout","text":"🔧 Installing packages (Colab-safe versions)...\n✅ torch>=2.1.0 installed\n✅ transformers>=4.53.0 installed\n✅ datasets>=2.18.0 installed\n✅ peft>=0.8.0 installed\n✅ trl>=0.7.0 installed\n✅ accelerate>=0.25.0 installed\n✅ huggingface_hub>=0.21.4 installed\n✅ feedparser>=6.0.11 installed\n✅ ddgs>=1.0.5 installed\n✅ requests installed\n✅ bitsandbytes>=0.41.3 installed\n📦 Importing libraries...\n","output_type":"stream"},{"name":"stderr","text":"2025-07-21 12:05:39.197793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753099539.563227      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753099539.672742      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✅ Core imports successful\n✅ TRL SFTTrainer available\n🖥️ PyTorch: 2.6.0+cu124\n🚀 CUDA available: True\n💾 GPU memory: 15.8 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import time\nfrom datetime import datetime\nfrom typing import List, Dict\nimport os\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nimport torch\nfrom trl import SFTTrainer\n\n# Login to Hugging Face\nlogin(token=\"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\")\n\n# Read Hugging Face token from Kaggle secrets\nsecret_label = \"HF_TOKEN\"  # Make sure this label matches your secret name in Kaggle\nsecret_value = UserSecretsClient().get_secret(secret_label)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":464,"referenced_widgets":["b1584beee2d940e7a28af47326216eb1","2586e670e2064c4088b8292c163f3fd8","ed610d2c04a8491eae730a7d934376bc","cddd38ab17e7434c889bfee5a07c3066","aaf15ec2deac4d9d8036e12e0c6efbb6","738fd91e4ec546e3b5c4307ee4b15a25","dcea76d82d0a414e9d531f0c985d4a51","46822ca36b03484f8dbb299b0bd29047","10075daabc0f436385b8f901973ad1bf","d98a9e5631134d1ca79f4a088fb1d2fe","9909b105647643ce99a3dca470126164","ac6a6a27860e4fa29ccb80391c7281a2","b1a2ede69d754cdd818afced04e047f5","dc087ec9d297449a8db488f9e4c2be0f","180f379056dd46ada784b069361d3e56","cc87965dcec247589bce534aa9a4ec58","08a18e95924c4f668ad70d5a0f48e872","10c6c2be11c142abba0b35f568d629d5","3402d54c71e14fd98de4876a9ff9e0d1","8163945228c74b32b4ac4c218f100a60","265f3c12d14d4d9d9a059f4dd667f9cd","2a2f428e387043c4925b70368ce15ee8","dba476834bd748f4aed7d728f7b759bf","607b6687ff2d49f0a7140f94272633c1","f537b006a1224dc89fad7e72ef297d28","423d4281cc834105ad2f45c173e9274d","73f3a5be96084773ba00a2812599ad77","7bb6759853eb4553bc8b8c67e446ddc7","7d754603fc8249f0b56e30584a2a917f","b37a45940b914c9c8733961720f7cb90","0ca718a74a024d879bccaea944bc3ffb","14fad5e48ab74dd0a60369edfb21d8e5","f45882a6628d4f3d9c577bc1f9676cf2","8d6acdd5112645ff8f6275d658527f41","a6bf8b486ef0447b95bc0793d3411f14","829ccde7455c4aa7bdbd9839cecb6817","2c8e6f7c4d434782be2e713ba835f9d9","9011dacc9d2e409aa43083fcabfb7d98","faf9b3ac846644df8de044cb574ac986","2f0777cc745d4e42b7f2fb8e53a187f7","9e294475bdbe49688a8dad2c4146652d","78f6c31d1a0f4f619b7c0c29d290d717","33888beee24a44afa08d20aefef61c01","2662786c46c247fc89cdb3025b357ea2"]},"id":"9AydlbDnsQ9A","outputId":"32044f7b-2315-437f-d439-d9e73ba994fa","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T12:05:59.878755Z","iopub.execute_input":"2025-07-21T12:05:59.878988Z","iopub.status.idle":"2025-07-21T12:06:00.155550Z","shell.execute_reply.started":"2025-07-21T12:05:59.878972Z","shell.execute_reply":"2025-07-21T12:06:00.155035Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# --- Config ---\nMODEL_NAME = \"HuggingFaceTB/SmolLM3-3B\"\nBATCH_SIZE = 4\nLR = 2e-5\nEPOCHS = 2\nUSE_4BIT = True\nMAX_SAMPLES = 1000  # Total samples for quick fine-tuning\nCUTOFF_DATE = datetime(2025, 7, 20)\nOFFLOAD_DIR = \"/content/offload\"\n\n# --- Tokenizer & Model ---\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\n# --- Quantization Config ---\nfrom transformers import BitsAndBytesConfig\nbnb_config = {\n    \"load_in_4bit\": USE_4BIT,\n    \"bnb_4bit_compute_dtype\": torch.float16,\n    \"bnb_4bit_use_double_quant\": True,\n    \"bnb_4bit_quant_type\": \"nf4\",\n}\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    offload_folder = OFFLOAD_DIR,\n    quantization_config=bnb_config,\n    trust_remote_code=True\n)\n\n# --- QLoRA ---\nmodel = prepare_model_for_kbit_training(model)\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"]\n)\nmodel = get_peft_model(model, lora_config)\n\n# --- Dataset Preparation ---\ndataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n\n# --- Training Setup ---\ntraining_args = TrainingArguments(\n    output_dir=\"./smollm3-qlora-ft\",\n    per_device_train_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    save_strategy=\"epoch\",\n    learning_rate=LR,\n    bf16=True,\n    logging_steps=10,\n    report_to=\"none\"\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    args=training_args\n)\n\n# --- Fine-Tune ---\ntrainer.train()\ntrainer.save_model(\"./smollm3-qlora-ft\")\nprint(\"✅ Fine-tuning complete.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":464,"referenced_widgets":["b1584beee2d940e7a28af47326216eb1","2586e670e2064c4088b8292c163f3fd8","ed610d2c04a8491eae730a7d934376bc","cddd38ab17e7434c889bfee5a07c3066","aaf15ec2deac4d9d8036e12e0c6efbb6","738fd91e4ec546e3b5c4307ee4b15a25","dcea76d82d0a414e9d531f0c985d4a51","46822ca36b03484f8dbb299b0bd29047","10075daabc0f436385b8f901973ad1bf","d98a9e5631134d1ca79f4a088fb1d2fe","9909b105647643ce99a3dca470126164","ac6a6a27860e4fa29ccb80391c7281a2","b1a2ede69d754cdd818afced04e047f5","dc087ec9d297449a8db488f9e4c2be0f","180f379056dd46ada784b069361d3e56","cc87965dcec247589bce534aa9a4ec58","08a18e95924c4f668ad70d5a0f48e872","10c6c2be11c142abba0b35f568d629d5","3402d54c71e14fd98de4876a9ff9e0d1","8163945228c74b32b4ac4c218f100a60","265f3c12d14d4d9d9a059f4dd667f9cd","2a2f428e387043c4925b70368ce15ee8","dba476834bd748f4aed7d728f7b759bf","607b6687ff2d49f0a7140f94272633c1","f537b006a1224dc89fad7e72ef297d28","423d4281cc834105ad2f45c173e9274d","73f3a5be96084773ba00a2812599ad77","7bb6759853eb4553bc8b8c67e446ddc7","7d754603fc8249f0b56e30584a2a917f","b37a45940b914c9c8733961720f7cb90","0ca718a74a024d879bccaea944bc3ffb","14fad5e48ab74dd0a60369edfb21d8e5","f45882a6628d4f3d9c577bc1f9676cf2","8d6acdd5112645ff8f6275d658527f41","a6bf8b486ef0447b95bc0793d3411f14","829ccde7455c4aa7bdbd9839cecb6817","2c8e6f7c4d434782be2e713ba835f9d9","9011dacc9d2e409aa43083fcabfb7d98","faf9b3ac846644df8de044cb574ac986","2f0777cc745d4e42b7f2fb8e53a187f7","9e294475bdbe49688a8dad2c4146652d","78f6c31d1a0f4f619b7c0c29d290d717","33888beee24a44afa08d20aefef61c01","2662786c46c247fc89cdb3025b357ea2"]},"id":"9AydlbDnsQ9A","outputId":"32044f7b-2315-437f-d439-d9e73ba994fa","trusted":true,"execution":{"iopub.status.busy":"2025-07-21T12:15:49.224609Z","iopub.execute_input":"2025-07-21T12:15:49.224907Z","iopub.status.idle":"2025-07-21T15:32:21.075860Z","shell.execute_reply.started":"2025-07-21T12:15:49.224886Z","shell.execute_reply":"2025-07-21T15:32:21.075059Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86313354c4b2402c991c92413cfbf4c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b57cca615a7941e4a8a2245f4446167e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/289 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e26a0e5c5df54d84991e32ba9d8f1919"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f465b5bdb5246a08e066d37f1328786"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"594020a82ad94e579cb1432a7160ae26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8873acb619847d3b66e9537d2f72add"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"422448b0c3264a17ad821e8d5b73b0b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d77cd4475d8b49b28a7fe5d6e51977a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7a498afdf964de8bf19785b8f8d368f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b9eb225ad5f4a118586fd7ff244b458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/182 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"017aac91d17e4ee1bc03f823c33a967c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c1cefa4ea654508be07950e88fc94b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-9ad84bb9cf65a4(…):   0%|          | 0.00/967k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89404ba3a2f24b1396b34bbadfb734e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"121bfaefa3ac4c8a8188e7f423e4e881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a97b408ed8274ff9aca6883f454d7f68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e679747e9877412c9a64a355a831e235"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7315c6d8c930401f8b4296dbb2dfda1f"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 3:15:01, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.500000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.461900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.446800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.350300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.465100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.377500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.338600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.454200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.600700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.274000</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.407100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.212900</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.284100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.404100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.219900</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.347200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.350400</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.338500</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.332400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.344700</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.242100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.235600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.421900</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.319500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.218700</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.316300</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.188500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.321200</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.290600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.258000</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.333900</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.216900</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.210800</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.386200</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.257000</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.284400</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.384100</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.195500</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.225000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.326300</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.501200</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.319500</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.328700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.338900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.397700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.214100</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.249800</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.271500</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.250300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.163400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"✅ Fine-tuning complete.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"repo_id = \"soupstick/smollm3-qlora-ft\"\nmodel.push_to_hub(repo_id, private=True, token=os.getenv(\"HF_TOKEN\"))\ntokenizer.push_to_hub(repo_id, private=True, token=os.getenv(\"HF_TOKEN\"))\nprint(\"✅ Model and tokenizer pushed to\", repo_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-21T15:36:08.717769Z","iopub.execute_input":"2025-07-21T15:36:08.718063Z","iopub.status.idle":"2025-07-21T15:36:18.212087Z","shell.execute_reply.started":"2025-07-21T15:36:08.718041Z","shell.execute_reply":"2025-07-21T15:36:18.211393Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/60.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8f9c720af3f486abe812f13592b9602"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c5913719e764610aa0e13118bcf100c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e844be6a70d6415baf0d27d43af303ce"}},"metadata":{}},{"name":"stdout","text":"✅ Model and tokenizer pushed to soupstick/smollm3-qlora-ft\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Fine Tune V3","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}